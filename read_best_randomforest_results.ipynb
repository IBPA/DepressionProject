{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "# import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import click\n",
    "from typing import Union\n",
    "from sklearn.metrics import precision_score\n",
    "import sklearn\n",
    "\n",
    "from msap.modeling.model_evaluation.statistics import (\n",
    "    get_embedded_data,\n",
    "    get_selected_features,\n",
    "    get_curve_metrics,\n",
    "    get_curve_metrics_test,\n",
    "    get_training_statistics,\n",
    "    get_baseline_training_statistics,\n",
    "    get_validation_statistics,\n",
    "    get_baseline_validation_statistics,\n",
    "    get_testing_statistics,\n",
    "    get_baseline_testing_statistics,\n",
    "    get_similarity_matrix)\n",
    "from msap.explanatory_analysis import get_pairwise_correlation\n",
    "from msap.utils import (\n",
    "    ClassifierHandler,\n",
    "    load_X_and_y,\n",
    "    KFold_by_feature)\n",
    "from msap.utils.plot import (\n",
    "    plot_heatmap,\n",
    "    plot_embedded_scatter,\n",
    "    plot_rfe_line,\n",
    "    plot_rfe_line_detailed,\n",
    "    plot_curves,\n",
    "    plot_confusion_matrix)\n",
    "from msap.modeling.configs import (\n",
    "    ModelSelectionConfig)\n",
    "\n",
    "\n",
    "METHODS_PC = ['pearson', 'spearman', 'kendall']\n",
    "METHODS_EMBEDDING = ['tsne', 'pca']\n",
    "METHODS_CURVE = ['pr', 'roc']\n",
    "CLASSIFIER_MODES = [\n",
    "    'decisiontreeclassifier',\n",
    "    'gaussiannb',\n",
    "    'multinomialnb',\n",
    "    'svc',\n",
    "    'adaboostclassifier',\n",
    "    'randomforestclassifier',\n",
    "    'mlpclassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_selection_result(ms_result: tuple) -> list:\n",
    "    \"\"\"Parse the model selection result tuple and get the best models.\n",
    "\n",
    "    Args:\n",
    "        ms_result: Model selection result tuple.\n",
    "\n",
    "    Returns:\n",
    "        List of best model and statistics for each classifiers.\n",
    "\n",
    "    \"\"\"\n",
    "    candidates, _ = ms_result\n",
    "    candidates = [(i, c, cv['best']) for i, c, cv in candidates]\n",
    "\n",
    "    f1s_mean = []\n",
    "    for i, c, cv_best in candidates:\n",
    "        # Iterate over splits to calculate average F1 score.\n",
    "        f1s = [cv_best[f'split_{j}']['f1'] for j in range(len(cv_best) - 1)]\n",
    "        f1s_mean += [np.mean(np.nan_to_num(f1s))]\n",
    "\n",
    "    candidates = list(zip(candidates, f1s_mean))\n",
    "    candidates = sorted(candidates, key=lambda x: x[1], reverse=True) # sorts so max is first\n",
    "\n",
    "    best_candidate_per_clf = []\n",
    "    for clf in CLASSIFIER_MODES:\n",
    "        for (i, c, cv_best), f1_mean in candidates:\n",
    "            if c[3] == clf:\n",
    "                if cv_best['param'] is not None:\n",
    "                    cv_best['param'] = {k.split('__')[-1]: v\n",
    "                                        for k, v in cv_best['param'].items()}\n",
    "\n",
    "                best_candidate_per_clf += [((i, c, cv_best), f1_mean)]\n",
    "                break # break to get the max\n",
    "\n",
    "    return best_candidate_per_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "path_input_model_selection_result = './output/pval_filter_60_MVI/output_12to18_yesmental/results.pkl'\n",
    "path_input_preprocessed_data_dir = './output/pval_filter_60_MVI/output_12to18_yesmental/preprocessed'\n",
    "path_output_dir = './output/pval_filter_60_MVI/output_12to18_yesmental/'\n",
    "feature_label = 'y12to18_Dep_YN_216m'\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_output_dir):\n",
    "    os.mkdir(path_output_dir)\n",
    "\n",
    "model_selection_result = None\n",
    "with open(path_input_model_selection_result, 'rb') as f:\n",
    "    model_selection_result = pickle.load(f)\n",
    "\n",
    "#print(model_selection_result)\n",
    "best_candidate_per_clf = parse_model_selection_result(\n",
    "    model_selection_result)\n",
    "best_candidate = max(best_candidate_per_clf, key=lambda x: x[1])\n",
    "_, best_combination, best_cv_result = best_candidate[0]\n",
    "best_scale_mode, best_impute_mode, best_outlier_mode, best_clf \\\n",
    "    = best_combination\n",
    "\n",
    "# print(best_combination)\n",
    "#pd.DataFrame(best_candidate_per_clf).to_csv(\n",
    "#    f\"{path_output_dir}/best_clfs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_selection_grid_search_results_for_best(ms_result: tuple, best_combination: tuple) -> list:\n",
    "    \"\"\"Parse the model selection result tuple and get the all models.\n",
    "\n",
    "    Args:\n",
    "        ms_result: Model selection result tuple.\n",
    "\n",
    "    Returns:\n",
    "        List of best model and statistics for each classifiers.\n",
    "\n",
    "    \"\"\"\n",
    "    candidates, _ = ms_result\n",
    "    # index, classifier, cv_result\n",
    "    # no longer want best, just want all?\n",
    "    candidates = [(i, c, cv) for i, c, cv in candidates]\n",
    "\n",
    "    grid_results = []\n",
    "    f1s_mean = []\n",
    "    for i, c, cv in candidates:\n",
    "        if c == best_combination:\n",
    "            # parse every grid search result\n",
    "            for key in cv:\n",
    "                # Iterate over splits to calculate average F1 score for clf\n",
    "                result = cv[key]\n",
    "                f1s = [result[f'split_{j}']['f1'] for j in range(len(result) - 1)]\n",
    "                grid_results += [(i, c, result)]\n",
    "                f1s_mean += [np.mean(np.nan_to_num(f1s))]\n",
    "\n",
    "    candidates = list(zip(grid_results, f1s_mean))\n",
    "    # candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    all_candidates_of_combination = []\n",
    "    for (i, c, cv), f1_mean in candidates:\n",
    "        if c == best_combination:\n",
    "            if cv['param'] is not None:\n",
    "                # get name of parameter (last word after '__') and value\n",
    "                cv['param'] = {k.split('__')[-1]: v\n",
    "                                    for k, v in cv['param'].items()}\n",
    "\n",
    "            all_candidates_of_combination += [((i, c, cv, result), f1_mean)]\n",
    "\n",
    "    return all_candidates_of_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grid_search_results_for_best_combination = parse_model_selection_grid_search_results_for_best(\n",
    "    model_selection_result, best_combination)\n",
    "# all_grid_search_results_for_best_combination"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe8a4c17498bfb38ff1297e9cf3182f98fb96035d105efd51d0d5c4b40f5a2b2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.depressionnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
