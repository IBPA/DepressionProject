{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "# import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import click\n",
    "from typing import Union\n",
    "from sklearn.metrics import precision_score\n",
    "import sklearn\n",
    "\n",
    "from msap.modeling.model_evaluation.statistics import (\n",
    "    get_embedded_data,\n",
    "    get_selected_features,\n",
    "    get_curve_metrics,\n",
    "    get_curve_metrics_test,\n",
    "    get_training_statistics,\n",
    "    get_baseline_training_statistics,\n",
    "    get_validation_statistics,\n",
    "    get_baseline_validation_statistics,\n",
    "    get_testing_statistics,\n",
    "    get_baseline_testing_statistics,\n",
    "    get_similarity_matrix)\n",
    "from msap.explanatory_analysis import get_pairwise_correlation\n",
    "from msap.utils import (\n",
    "    ClassifierHandler,\n",
    "    load_X_and_y,\n",
    "    KFold_by_feature)\n",
    "from msap.utils.plot import (\n",
    "    plot_heatmap,\n",
    "    plot_embedded_scatter,\n",
    "    plot_rfe_line,\n",
    "    plot_rfe_line_detailed,\n",
    "    plot_curves,\n",
    "    plot_confusion_matrix)\n",
    "from msap.modeling.configs import (\n",
    "    ModelSelectionConfig)\n",
    "\n",
    "\n",
    "METHODS_PC = ['pearson', 'spearman', 'kendall']\n",
    "METHODS_EMBEDDING = ['tsne', 'pca']\n",
    "METHODS_CURVE = ['pr', 'roc']\n",
    "CLASSIFIER_MODES = [\n",
    "    'decisiontreeclassifier',\n",
    "    'gaussiannb',\n",
    "    'multinomialnb',\n",
    "    'svc',\n",
    "    'adaboostclassifier',\n",
    "    'randomforestclassifier',\n",
    "    'mlpclassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_selection_result(ms_result: tuple) -> list:\n",
    "    \"\"\"Parse the model selection result tuple and get the best models.\n",
    "\n",
    "    Args:\n",
    "        ms_result: Model selection result tuple.\n",
    "\n",
    "    Returns:\n",
    "        List of best model and statistics for each classifiers.\n",
    "\n",
    "    \"\"\"\n",
    "    candidates, _ = ms_result\n",
    "    candidates = [(i, c, cv['best']) for i, c, cv in candidates]\n",
    "\n",
    "    f1s_mean = []\n",
    "    for i, c, cv_best in candidates:\n",
    "        # Iterate over splits to calculate average F1 score.\n",
    "        f1s = [cv_best[f'split_{j}']['f1'] for j in range(len(cv_best) - 1)]\n",
    "        f1s_mean += [np.mean(np.nan_to_num(f1s))]\n",
    "\n",
    "    candidates = list(zip(candidates, f1s_mean))\n",
    "    candidates = sorted(candidates, key=lambda x: x[1], reverse=True) # sorts so max is first\n",
    "\n",
    "    best_candidate_per_clf = []\n",
    "    for clf in CLASSIFIER_MODES:\n",
    "        for (i, c, cv_best), f1_mean in candidates:\n",
    "            if c[3] == clf:\n",
    "                if cv_best['param'] is not None:\n",
    "                    cv_best['param'] = {k.split('__')[-1]: v\n",
    "                                        for k, v in cv_best['param'].items()}\n",
    "\n",
    "                best_candidate_per_clf += [((i, c, cv_best), f1_mean)]\n",
    "                break # break to get the max\n",
    "\n",
    "    return best_candidate_per_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "path_input_model_selection_result = './output/pval_filter_60_MVI/output_12to18_yesmental/results.pkl'\n",
    "path_input_preprocessed_data_dir = './output/pval_filter_60_MVI/output_12to18_yesmental/preprocessed'\n",
    "path_output_dir = './output/pval_filter_60_MVI/output_12to18_yesmental/'\n",
    "feature_label = 'y12to18_Dep_YN_216m'\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_output_dir):\n",
    "    os.mkdir(path_output_dir)\n",
    "\n",
    "model_selection_result = None\n",
    "with open(path_input_model_selection_result, 'rb') as f:\n",
    "    model_selection_result = pickle.load(f)\n",
    "\n",
    "#print(model_selection_result)\n",
    "best_candidate_per_clf = parse_model_selection_result(\n",
    "    model_selection_result)\n",
    "best_candidate = max(best_candidate_per_clf, key=lambda x: x[1])\n",
    "_, best_combination, best_cv_result = best_candidate[0]\n",
    "best_scale_mode, best_impute_mode, best_outlier_mode, best_clf \\\n",
    "    = best_combination\n",
    "\n",
    "# print(best_combination)\n",
    "#pd.DataFrame(best_candidate_per_clf).to_csv(\n",
    "#    f\"{path_output_dir}/best_clfs.csv\")\n",
    "# model_selection_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_selection_grid_search_results_for_best(ms_result: tuple, best_combination: tuple) -> list:\n",
    "    \"\"\"Parse the model selection result tuple and get the all models.\n",
    "\n",
    "    Args:\n",
    "        ms_result: Model selection result tuple.\n",
    "\n",
    "    Returns:\n",
    "        List of best model and statistics for each classifiers.\n",
    "\n",
    "    \"\"\"\n",
    "    candidates, _ = ms_result\n",
    "    # index, classifier, cv_result\n",
    "    # no longer want best, just want all?\n",
    "    candidates = [(i, c, cv) for i, c, cv in candidates]\n",
    "\n",
    "    grid_results = []\n",
    "    f1s_mean = []\n",
    "    for i, c, cv in candidates:\n",
    "        if c == best_combination:\n",
    "            # parse every grid search result\n",
    "            for key in cv:\n",
    "                # Iterate over splits to calculate average F1 score for clf\n",
    "                result = cv[key]\n",
    "                f1s = [result[f'split_{j}']['f1'] for j in range(len(result) - 1)]\n",
    "                grid_results += [((i, key), c, result)]\n",
    "                f1s_mean += [np.mean(np.nan_to_num(f1s))]\n",
    "\n",
    "    candidates = list(zip(grid_results, f1s_mean))\n",
    "    # candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    all_candidates_of_combination = []\n",
    "    for (i, c, cv), f1_mean in candidates:\n",
    "        if c == best_combination:\n",
    "            if cv['param'] is not None:\n",
    "                # get name of parameter (last word after '__') and value\n",
    "                cv['param'] = {k.split('__')[-1]: v\n",
    "                                    for k, v in cv['param'].items()}\n",
    "\n",
    "            all_candidates_of_combination += [((i, c, cv), f1_mean)]\n",
    "\n",
    "    return all_candidates_of_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grid_search_results_for_best_combination = parse_model_selection_grid_search_results_for_best(\n",
    "    model_selection_result, best_combination)\n",
    "# all_grid_search_results_for_best_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe of current results\n",
    "i_gridis = []\n",
    "cs = []\n",
    "params = []\n",
    "splits = []\n",
    "f1s = []\n",
    "for ((i, grid_i), c, cv), f1_mean in all_grid_search_results_for_best_combination:\n",
    "    #print(f\"{i} {grid_i} {c} {cv} {f1_mean}\")\n",
    "    #break\n",
    "    i_gridis += [(i, grid_i)]\n",
    "    cs += [c]\n",
    "    params += [cv['param']]\n",
    "    splits += [[cv[f'split_{j}'] for j in range(len(cv) - 1)]]\n",
    "    f1s += [f1_mean]\n",
    "\n",
    "grids = {'i_gridis': i_gridis, 'cs': cs, 'params': params, 'splits': splits, 'f1s': f1s}\n",
    "df = pd.DataFrame(grids)\n",
    "\n",
    "#df['params'].apply(pd.Series)\n",
    "df = pd.concat([df, df['params'].apply(pd.Series)], axis=1)\n",
    "df = df.drop(columns='params')\n",
    "#df\n",
    "#df.to_csv(f\"{path_output_dir}/best_clf_grid_search.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similar(row, best_params):\n",
    "    \"\"\"Check if the current row is similar to the best params.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Current row to check if similar to best params\n",
    "        best_params (dict): Best params\n",
    "\n",
    "    Returns:\n",
    "        True if similar, False otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "    num = 0\n",
    "    for key in best_params:\n",
    "        if row[key] != best_params[key]:\n",
    "            num += 1\n",
    "    if num > 1:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 100, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "# filter to only include data from params that are only 1 different from best params\n",
    "best_params = best_cv_result['param']\n",
    "print(best_params)\n",
    "\n",
    "rows_similar_params = [idx for idx, row in df.iterrows() if check_similar(row, best_params)]\n",
    "df_similar_params = df.iloc[rows_similar_params]\n",
    "# df_similar_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe8a4c17498bfb38ff1297e9cf3182f98fb96035d105efd51d0d5c4b40f5a2b2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.depressionnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
