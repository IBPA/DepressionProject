{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "# import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import click\n",
    "from typing import Union\n",
    "from sklearn.metrics import precision_score\n",
    "import sklearn\n",
    "\n",
    "from msap.modeling.model_evaluation.statistics import (\n",
    "    get_embedded_data,\n",
    "    get_selected_features,\n",
    "    get_curve_metrics,\n",
    "    get_curve_metrics_test,\n",
    "    get_training_statistics,\n",
    "    get_baseline_training_statistics,\n",
    "    get_validation_statistics,\n",
    "    get_baseline_validation_statistics,\n",
    "    get_testing_statistics,\n",
    "    get_baseline_testing_statistics,\n",
    "    get_similarity_matrix)\n",
    "from msap.explanatory_analysis import get_pairwise_correlation\n",
    "from msap.utils import (\n",
    "    ClassifierHandler,\n",
    "    load_X_and_y,\n",
    "    KFold_by_feature)\n",
    "from msap.utils.plot import (\n",
    "    plot_heatmap,\n",
    "    plot_embedded_scatter,\n",
    "    plot_rfe_line,\n",
    "    plot_rfe_line_detailed,\n",
    "    plot_curves,\n",
    "    plot_confusion_matrix)\n",
    "from msap.modeling.configs import (\n",
    "    ModelSelectionConfig)\n",
    "\n",
    "\n",
    "METHODS_PC = ['pearson', 'spearman', 'kendall']\n",
    "METHODS_EMBEDDING = ['tsne', 'pca']\n",
    "METHODS_CURVE = ['pr', 'roc']\n",
    "CLASSIFIER_MODES = [\n",
    "    'decisiontreeclassifier',\n",
    "    'gaussiannb',\n",
    "    'multinomialnb',\n",
    "    'svc',\n",
    "    'adaboostclassifier',\n",
    "    'randomforestclassifier',\n",
    "    'mlpclassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_selection_result(ms_result: tuple) -> list:\n",
    "    \"\"\"Parse the model selection result tuple and get the best models.\n",
    "\n",
    "    Args:\n",
    "        ms_result: Model selection result tuple.\n",
    "\n",
    "    Returns:\n",
    "        List of best model and statistics for each classifiers.\n",
    "\n",
    "    \"\"\"\n",
    "    candidates, _ = ms_result\n",
    "    candidates = [(i, c, cv['best']) for i, c, cv in candidates]\n",
    "\n",
    "    f1s_mean = []\n",
    "    for i, c, cv_best in candidates:\n",
    "        # Iterate over splits to calculate average F1 score.\n",
    "        f1s = [cv_best[f'split_{j}']['f1'] for j in range(len(cv_best) - 1)]\n",
    "        f1s_mean += [np.mean(np.nan_to_num(f1s))]\n",
    "\n",
    "    candidates = list(zip(candidates, f1s_mean))\n",
    "    candidates = sorted(candidates, key=lambda x: x[1], reverse=True) # sorts so max is first\n",
    "\n",
    "    best_candidate_per_clf = []\n",
    "    for clf in CLASSIFIER_MODES:\n",
    "        for (i, c, cv_best), f1_mean in candidates:\n",
    "            if c[3] == clf:\n",
    "                if cv_best['param'] is not None:\n",
    "                    cv_best['param'] = {k.split('__')[-1]: v\n",
    "                                        for k, v in cv_best['param'].items()}\n",
    "\n",
    "                best_candidate_per_clf += [((i, c, cv_best), f1_mean)]\n",
    "                break # break to get the max\n",
    "\n",
    "    return best_candidate_per_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "path_input_model_selection_result = './output/pval_filter_60_MVI/output_12to18_yesmental/results.pkl'\n",
    "path_input_preprocessed_data_dir = './output/pval_filter_60_MVI/output_12to18_yesmental/preprocessed'\n",
    "path_output_dir = './output/pval_filter_60_MVI/output_12to18_yesmental/'\n",
    "feature_label = 'y12to18_Dep_YN_216m'\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_output_dir):\n",
    "    os.mkdir(path_output_dir)\n",
    "\n",
    "model_selection_result = None\n",
    "with open(path_input_model_selection_result, 'rb') as f:\n",
    "    model_selection_result = pickle.load(f)\n",
    "\n",
    "#print(model_selection_result)\n",
    "best_candidate_per_clf = parse_model_selection_result(\n",
    "    model_selection_result)\n",
    "best_candidate = max(best_candidate_per_clf, key=lambda x: x[1])\n",
    "_, best_combination, best_cv_result = best_candidate[0]\n",
    "best_scale_mode, best_impute_mode, best_outlier_mode, best_clf \\\n",
    "    = best_combination\n",
    "\n",
    "# print(best_combination)\n",
    "#pd.DataFrame(best_candidate_per_clf).to_csv(\n",
    "#    f\"{path_output_dir}/best_clfs.csv\")\n",
    "# model_selection_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_selection_grid_search_results_for_best(ms_result: tuple, best_combination: tuple) -> list:\n",
    "    \"\"\"Parse the model selection result tuple and get the all models.\n",
    "\n",
    "    Args:\n",
    "        ms_result: Model selection result tuple.\n",
    "\n",
    "    Returns:\n",
    "        List of best model and statistics for each classifiers.\n",
    "\n",
    "    \"\"\"\n",
    "    candidates, _ = ms_result\n",
    "    # index, classifier, cv_result\n",
    "    # no longer want best, just want all?\n",
    "    candidates = [(i, c, cv) for i, c, cv in candidates]\n",
    "\n",
    "    grid_results = []\n",
    "    f1s_mean = []\n",
    "    for i, c, cv in candidates:\n",
    "        if c == best_combination:\n",
    "            # parse every grid search result\n",
    "            for key in cv:\n",
    "                # Iterate over splits to calculate average F1 score for clf\n",
    "                result = cv[key]\n",
    "                f1s = [result[f'split_{j}']['f1'] for j in range(len(result) - 1)]\n",
    "                grid_results += [((i, key), c, result)]\n",
    "                f1s_mean += [np.mean(np.nan_to_num(f1s))]\n",
    "\n",
    "    candidates = list(zip(grid_results, f1s_mean))\n",
    "    # candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    all_candidates_of_combination = []\n",
    "    for (i, c, cv), f1_mean in candidates:\n",
    "        if c == best_combination:\n",
    "            if cv['param'] is not None:\n",
    "                # get name of parameter (last word after '__') and value\n",
    "                cv['param'] = {k.split('__')[-1]: v\n",
    "                                    for k, v in cv['param'].items()}\n",
    "\n",
    "            all_candidates_of_combination += [((i, c, cv), f1_mean)]\n",
    "\n",
    "    return all_candidates_of_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grid_search_results_for_best_combination = parse_model_selection_grid_search_results_for_best(\n",
    "    model_selection_result, best_combination)\n",
    "# all_grid_search_results_for_best_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe of current results\n",
    "i_gridis = []\n",
    "cs = []\n",
    "params = []\n",
    "splits = []\n",
    "f1s = []\n",
    "for ((i, grid_i), c, cv), f1_mean in all_grid_search_results_for_best_combination:\n",
    "    #print(f\"{i} {grid_i} {c} {cv} {f1_mean}\")\n",
    "    #break\n",
    "    i_gridis += [(i, grid_i)]\n",
    "    cs += [c]\n",
    "    params += [cv['param']]\n",
    "    splits += [[cv[f'split_{j}'] for j in range(len(cv) - 1)]]\n",
    "    f1s += [f1_mean]\n",
    "\n",
    "grids = {'i_gridis': i_gridis, 'cs': cs, 'params': params, 'splits': splits, 'f1s': f1s}\n",
    "df = pd.DataFrame(grids)\n",
    "\n",
    "#df['params'].apply(pd.Series)\n",
    "df = pd.concat([df, df['params'].apply(pd.Series)], axis=1)\n",
    "df = df.drop(columns='params')\n",
    "#df\n",
    "#df.to_csv(f\"{path_output_dir}/best_clf_grid_search.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>criterion</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>random_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>125</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>entropy</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>425</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>entropy</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>450</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>entropy</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>475</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>entropy</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     criterion  min_samples_leaf  min_samples_split  n_estimators  \\\n",
       "0         gini                 1                  2            25   \n",
       "1         gini                 1                  2            50   \n",
       "2         gini                 1                  2            75   \n",
       "3         gini                 1                  2           100   \n",
       "4         gini                 1                  2           125   \n",
       "...        ...               ...                ...           ...   \n",
       "996    entropy                 9                 10           425   \n",
       "997    entropy                 9                 10           450   \n",
       "998    entropy                 9                 10           475   \n",
       "999    entropy                 9                 10           500   \n",
       "1000   entropy                 3                 10           100   \n",
       "\n",
       "      random_state  \n",
       "0               42  \n",
       "1               42  \n",
       "2               42  \n",
       "3               42  \n",
       "4               42  \n",
       "...            ...  \n",
       "996             42  \n",
       "997             42  \n",
       "998             42  \n",
       "999             42  \n",
       "1000            42  \n",
       "\n",
       "[1001 rows x 5 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe8a4c17498bfb38ff1297e9cf3182f98fb96035d105efd51d0d5c4b40f5a2b2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.depressionnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
